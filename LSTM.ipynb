{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/illhyhl1111/SNU_ML2019/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-HG7MfXo78",
        "colab_type": "text"
      },
      "source": [
        "# original code from : https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrW_p2qiPiwM",
        "colab_type": "code",
        "outputId": "c9fb0894-8900-4c71-e203-1bd81bee82b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4604466370>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJptVh0PDO5",
        "colab_type": "text"
      },
      "source": [
        "# LSTM 예시\n",
        " * input dimension과 output dimension이 3인 LSTMTagger\n",
        " * lstm(input, hidden) 은 해당 input과 주어진 현재 hidden state를 받아,\n",
        "   output과 다음 hidden state를 리턴한다.\n",
        " * lstm(inputs, hidden) 은 해당 input sequence를 주어진 현재 state에 차례로 넣어,\n",
        "   output sequence와 최종 hidden state를 리턴한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slx-CUFqVkKc",
        "colab_type": "code",
        "outputId": "0d6f483a-b7ef-49a6-ca9d-3a9a01d9be69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print(out)\n",
        "    print(hidden)\n",
        "\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.1177, -0.0202,  0.1327]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.1177, -0.0202,  0.1327]]], grad_fn=<StackBackward>), tensor([[[-0.2304, -0.0818,  0.6783]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.0806,  0.1281,  0.1871]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.0806,  0.1281,  0.1871]]], grad_fn=<StackBackward>), tensor([[[-0.2258,  0.2400,  0.4334]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.2598,  0.1070,  0.1443]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.2598,  0.1070,  0.1443]]], grad_fn=<StackBackward>), tensor([[[-0.4415,  0.2775,  0.3539]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.6068,  0.0427,  0.0903]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.6068,  0.0427,  0.0903]]], grad_fn=<StackBackward>), tensor([[[-0.8996,  0.2223,  0.4447]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.5103,  0.0466,  0.1605]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.5103,  0.0466,  0.1605]]], grad_fn=<StackBackward>), tensor([[[-0.7977,  0.1429,  0.3695]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.2330, -0.0432,  0.2007]],\n",
            "\n",
            "        [[-0.1405,  0.0438,  0.2506]],\n",
            "\n",
            "        [[-0.3252,  0.0824,  0.1883]],\n",
            "\n",
            "        [[-0.6555,  0.0408,  0.1015]],\n",
            "\n",
            "        [[-0.5362,  0.0468,  0.1725]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.5362,  0.0468,  0.1725]]], grad_fn=<StackBackward>), tensor([[[-0.8540,  0.1412,  0.3962]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSIBT1HPEuv",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 예시\n",
        " * 각 단어의 품사를 추론하는 모델을 만드는 것을 목표로 한다.\n",
        " * 각 단어 별로 품사가 labeling된 데이터를 준비한다(아래 코드의 training_data).\n",
        " * 각 단어 혹은 품사 sequence를 숫자로 encoding한다(아래 코드의 prepare_sequence 함수).\n",
        " * 이 때, 단어를 숫자로 encoding할 때는 word_to_ix dictionary를 사용하고, 품사를 숫자로 encoding할 때는 tag_to_ix dictionary를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1fiAxoYVo-L",
        "colab_type": "code",
        "outputId": "54e61c04-c665-4fe8-8c59-a6a650b6e3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYz3ha0nQWV2",
        "colab_type": "text"
      },
      "source": [
        "# 품사 예측 LSTM 모델\n",
        " * 각 단어를 embedding_dim 차원 공간에 embedding하여, 해당 embedding을 LSTM의 input으로 사용한다.\n",
        " * LSTM은 hidden_dim 차원의 hidden state를 가지며, output인 품사 score는 hidden state에 fully connected layer를 적용하여 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R6nYw_oVtGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogcTXXfRbpJ",
        "colab_type": "text"
      },
      "source": [
        "# LSTM training\n",
        " * 아까 정의한 prepare_sequence 함수를 이용해 training data를 encoding하여 LSTM에 input으로 넣는다.\n",
        " * LSTM 모델에서 나온 tag score와 tag label와의 NLLLoss를 loss로써 사용하여 학습한다.\n",
        "  (https://pytorch.org/docs/stable/nn.html#nllloss 참조)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmvH2xgWRA1",
        "colab_type": "code",
        "outputId": "0ebdaf3c-0a44-4fe4-d6c3-0803f8e37101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print(tag_scores)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.3599, -0.9680, -1.0121],\n",
            "        [-1.2462, -0.9999, -1.0656],\n",
            "        [-1.3484, -0.9154, -1.0789],\n",
            "        [-1.4136, -0.8850, -1.0670],\n",
            "        [-1.4271, -0.8878, -1.0543]])\n",
            "tensor([[-0.0437, -3.4309, -4.5645],\n",
            "        [-4.1781, -0.0296, -4.2791],\n",
            "        [-3.4239, -2.8304, -0.0960],\n",
            "        [-0.0422, -4.5994, -3.4646],\n",
            "        [-4.8367, -0.0203, -4.4117]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPej6rp3S9PR",
        "colab_type": "text"
      },
      "source": [
        " * 각 단어에 대해 가장 score가 높은 index에 해당되는 품사가 LSTM이 추론한 품사이다.\n",
        " * 학습한 결과는 'The dog ate the apple'이라는 문장에 대해 'DET NN V DET NN'으로 품사를 추론하고, 'Everybody read that book'이라는 문장에 대해 'NN V DET NN'으로 품사를 추론함을 알 수 있다."
      ]
    }
  ]
}