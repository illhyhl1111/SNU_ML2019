{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/illhyhl1111/SNU_ML2019/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-HG7MfXo78",
        "colab_type": "text"
      },
      "source": [
        "# original code from : https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrW_p2qiPiwM",
        "colab_type": "code",
        "outputId": "21d5070e-7e6f-4c6a-9bd5-2c87a7a8b259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5cc21360f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJptVh0PDO5",
        "colab_type": "text"
      },
      "source": [
        "# LSTM 예시\n",
        " * input dimension과 output dimension이 3인 LSTMTagger\n",
        " * lstm(input, hidden) 은 해당 input과 주어진 현재 hidden state를 받아,\n",
        "   output과 다음 hidden state를 리턴한다.\n",
        " * lstm(inputs, hidden) 은 해당 input sequence를 주어진 현재 state에 차례로 넣어,\n",
        "   output sequence와 최종 hidden state를 리턴한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slx-CUFqVkKc",
        "colab_type": "code",
        "outputId": "eb15c88f-2856-426c-9e1a-0a8ade970402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print(out)\n",
        "    print(hidden)\n",
        "\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>), tensor([[[-1.0766,  0.0972, -0.5498]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>), tensor([[[-1.1552,  0.1214, -0.2974]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>), tensor([[[-1.0727,  0.1104, -0.2179]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>), tensor([[[-1.0530,  0.1836, -0.1731]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
            "\n",
            "        [[-0.3521,  0.1026, -0.2971]],\n",
            "\n",
            "        [[-0.3191,  0.0781, -0.1957]],\n",
            "\n",
            "        [[-0.1634,  0.0941, -0.1637]],\n",
            "\n",
            "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSIBT1HPEuv",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 예시\n",
        " * 각 단어의 품사를 추론하는 모델을 만드는 것을 목표로 한다.\n",
        " * 각 단어 별로 품사가 labeling된 데이터를 준비한다(아래 코드의 training_data).\n",
        " * 각 단어 혹은 품사 sequence를 숫자로 encoding한다(아래 코드의 prepare_sequence 함수).\n",
        " * 이 때, 단어를 숫자로 encoding할 때는 word_to_ix dictionary를 사용하고, 품사를 숫자로 encoding할 때는 tag_to_ix dictionary를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1fiAxoYVo-L",
        "colab_type": "code",
        "outputId": "57cf2241-eaa6-4660-add4-c58e5a5791fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYz3ha0nQWV2",
        "colab_type": "text"
      },
      "source": [
        "# 품사 예측 LSTM 모델\n",
        " * 각 단어를 embedding_dim 차원 공간에 embedding하여, 해당 embedding을 LSTM의 input으로 사용한다.\n",
        " * LSTM은 hidden_dim 차원의 hidden state를 가지며, output인 품사 score는 hidden state에 fully connected layer를 적용하여 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R6nYw_oVtGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogcTXXfRbpJ",
        "colab_type": "text"
      },
      "source": [
        "# LSTM training\n",
        " * 아까 정의한 prepare_sequence 함수를 이용해 training data를 encoding하여 LSTM에 input으로 넣는다.\n",
        " * LSTM 모델에서 나온 tag score와 tag label와의 NLLLoss를 loss로써 사용하여 학습한다.\n",
        "  (https://pytorch.org/docs/stable/nn.html#nllloss 참조)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmvH2xgWRA1",
        "colab_type": "code",
        "outputId": "1b97b44f-4800-436e-8db1-48c7124c2aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print('before training \\n : ', tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print('after training \\n : ', tag_scores)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before training \n",
            " :  tensor([[-1.1673, -1.2724, -0.8949],\n",
            "        [-1.0838, -1.3116, -0.9357],\n",
            "        [-0.9590, -1.3362, -1.0388],\n",
            "        [-1.0451, -1.3285, -0.9585],\n",
            "        [-1.0733, -1.3155, -0.9422]])\n",
            "after training \n",
            " :  tensor([[-0.0250, -4.1570, -4.7088],\n",
            "        [-4.5116, -0.0166, -5.2050],\n",
            "        [-3.8958, -4.8731, -0.0284],\n",
            "        [-0.0546, -3.2526, -4.2355],\n",
            "        [-4.2795, -0.0182, -5.4813]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPej6rp3S9PR",
        "colab_type": "text"
      },
      "source": [
        " * 각 단어에 대해 가장 score가 높은 index에 해당되는 품사가 LSTM이 추론한 품사이다.\n",
        " * 학습한 결과는 'The dog ate the apple'이라는 문장에 대해 'DET NN V DET NN'으로 품사를 추론하고, 'Everybody read that book'이라는 문장에 대해 'NN V DET NN'으로 품사를 추론함을 알 수 있다."
      ]
    }
  ]
}
